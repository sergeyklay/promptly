# This file is part of the Promptly.
#
# Copyright (C) 2023 Serghei Iakovlev <egrep@protonmail.ch>
#
# For the full copyright and license information, please view
# the LICENSE file that was distributed with this source code.

# Determines the host address on which the Flask server will listen for incoming
# connections. By default, Flask uses the value '127.0.0.1' (localhost), which
# means that the server will be accessible only on the local machine. If you
# want to make the server accessible to external clients, you can set
# FLASK_RUN_HOST to the value '0.0.0.0' or another IP address of your server.
FLASK_RUN_HOST=127.0.0.1

# Defines the port on which the Flask server will run. By default, Flask uses
# port 5000. You can change this port by setting a value for FLASK_RUN_PORT
# according to your requirements.
FLASK_RUN_PORT=5000

# The API key for authenticating with the OpenAI GPT service. This key provides
# access to the GPT API which is essential for sending requests to the OpenAI
# servers to generate responses based on the provided prompts. Ensure to keep
# this key secure as it can be used to incur charges on your OpenAI account. You
# can obtain your own API key by signing up at
# https://platform.openai.com/signup/ .
OPENAI_API_KEY='sk-...'

# Specifies the model to be used by OpenAI for processing requests. The value of
# OPENAI_MODEL should correspond to a specific version of the model provided by
# OpenAI. For instance, 'gpt-3.5-turbo-16k' refers to a specific configuration
# of the GPT-3.5 Turbo model with 16,000 training steps.
#
# The model specified here will affect the quality and the nature of the
# responses generated by the OpenAI API. Different models may have different
# performance characteristics, capabilities, and pricing. Therefore, it is
# crucial to choose a model that aligns well with the requirements of your
# application.
#
# You can obtain the list of available models and their specifications from the
# OpenAI documentation or by contacting OpenAI support.
#
# It's essential to note that using more advanced or larger models may incur
# higher costs on your OpenAI account. Therefore, ensure to review the pricing
# details of the models and choose one that provides a good balance between
# cost and performance for your use case.
#
# Example:
# OPENAI_MODEL='gpt-4-1106-preview'
# OPENAI_MODEL='gpt-4-vision-preview'
# OPENAI_MODEL='gpt-4'
# OPENAI_MODEL='gpt-4-0314'
# OPENAI_MODEL='gpt-4-0613'
# OPENAI_MODEL='gpt-4-32k'
# OPENAI_MODEL='gpt-4-32k-0314'
# OPENAI_MODEL='gpt-4-32k-0613
# OPENAI_MODEL='gpt-3.5-turbo-1106
# OPENAI_MODEL='gpt-3.5-turbo'
# OPENAI_MODEL='gpt-3.5-turbo-0301'
# OPENAI_MODEL='gpt-3.5-turbo-0613'
# OPENAI_MODEL='gpt-3.5-turbo-16k-0613'
OPENAI_MODEL='gpt-3.5-turbo-16k'

# Specifies the logging level for the OpenAI library. The value of OPENAI_LOG
# controls the verbosity of logging messages generated by the openai-python
# library during the interaction with the OpenAI API.
#
# Setting OPENAI_LOG to 'debug' will provide detailed debug information in the
# logs which can be extremely helpful for troubleshooting issues with the OpenAI
# API interactions. This includes, but is not limited to, the complete request
# and response data, including headers and payload. It is advisable to use this
# level with caution in a production environment as it may expose sensitive
# information in the logs.
#
# In a development environment, setting OPENAI_LOG to 'debug' or 'infoâ€™ could
# provide useful insights into the operation of the OpenAI library and the
# interaction with the OpenAI API. In a production environment, it might be more
# prudent to unset OPENAI_LOG reducing the volume of logged data and preserving
# resources.
#
# Example:
# OPENAI_LOG='debug'

# Specifies the maximum amount of time, in seconds, that the `threaded_execute`
# function will wait for the completion of the function it is executing in a
# separate thread. If the function does not complete within this time, a
# concurrent.futures.TimeoutError will be thrown, logged, and the function will
# be invoked again with the same arguments.
#
# The default value is 60 seconds, but you can adjust this according to the
# specific requirements of your environment by setting a different value for
# PROMPTLY_THREAD_TIMEOUT in this configuration file. This timeout setting helps
# in ensuring that the function does not hang indefinitely and consumes
# resources, while providing a mechanism to handle scenarios where the function
# takes longer than expected to execute.
#
# It's essential to set a reasonable timeout value to balance between giving the
# function enough time to complete its execution and preventing resource
# exhaustion in case of functions that might hang or take an excessive amount of
# time to finish.
#
# Example:
# PROMPTLY_THREAD_TIMEOUT=120  # Sets the timeout to 120 seconds.
PROMPTLY_THREAD_TIMEOUT=60

# Local database to use w/o Docker. Comment it out to use the default value.
# To see the default value refer to 'provider.DevelopmentConfig' in
# 'provider/config.py' file.
#
# DEV_DATABASE_URL=sqlite:////home/user/work/promptly/dev-sb.sqlite3

# In-memory database for testing purposes.
TEST_DATABASE_URL=sqlite://

# Production database.
# dialect[+driver]://user:password@host/dbname[?key=value..]
DATABASE_URL=mysql+pymysql://user:password@127.0.0.1:3306/db_name

# Local Variables:
# mode: conf-unix
# coding: utf-8-unix
# fill-column: 80
# eval: (display-fill-column-indicator-mode)
# End:
